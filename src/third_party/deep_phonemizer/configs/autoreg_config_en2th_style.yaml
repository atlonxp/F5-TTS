
paths:
  checkpoint_dir: checkpoints_autoreg_sm_en2th_style   # Directory to store model checkpoints and tensorboard, will be created if not existing.
  data_dir: datasets            # Directory to store processed data, will be created if not existing.

preprocessing:
  languages: ['phn']    # All languages in the dataset.

  # Text (grapheme) and phoneme symbols, either provide a string or list of strings.
  # Symbols in the dataset will be filtered according to these lists!
  text_symbols: ['', '.', 'AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0', 'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH', 'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1', 'EY2', 'F', 'G', 'HH', 'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW0', 'OW1', 'OW2', 'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH0', 'UH1', 'UH2', 'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH',"a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z",".",",","0","1","2","3","4","5","6","7","8","9","{","}",'"',"'","&"]
  
  phoneme_symbols: ['@0', '@1', '@2', '@3', '@4', '@5', '@@0', '@@1', '@@2', '@@3', '@@4', '@@5', 'a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'aa0', 'aa1', 'aa2', 'aa3', 'aa4', 'aa5', 'b', 'bl', 'br', 'c', 'c^', 'ch', 'ch^', 'd', 'd^', 'dr', 'e0', 'e1', 'e2', 'e3', 'e4', 'e5', 'ee0', 'ee1', 'ee2', 'ee3', 'ee4', 'ee5', 'f', 'f^', 'fl', 'fr', 'h', 'i0', 'i1', 'i2', 'i3', 'i4', 'i5', 'ia0', 'ia1', 'ia2', 'ia3', 'ia4', 'ii0', 'ii1', 'ii2', 'ii3', 'ii4', 'ii5', 'iia0', 'iia1', 'iia2', 'iia3', 'iia4', 'iia5', 'j', 'j^', 'jf^', 'js^', 'k', 'k^', 'kh', 'khl', 'khr', 'khw', 'kl', 'kr', 'ks^', 'kw', 'l', 'l^', 'ls^', 'm', 'm^', 'n', 'n^', 'ng', 'ng^', 'ns^', 'o0', 'o1', 'o2', 'o3', 'o4', 'o5', 'oo0', 'oo1', 'oo2', 'oo3', 'oo4', 'oo5', 'p', 'p^', 'ph', 'phl', 'phr', 'pl', 'pr', 'q0', 'q1', 'q2', 'q3', 'q4', 'q5', 'qq0', 'qq1', 'qq2', 'qq3', 'qq4', 'qq5', 'r', 's', 's^', 'sh', 't', 't^', 'th', 'tha', 'thr', 'tl', 'tr', 'ts^', 'u0', 'u1', 'u2', 'u3', 'u4', 'u5', 'ua0', 'ua2', 'ua3', 'uu0', 'uu1', 'uu2', 'uu3', 'uu4', 'uu5', 'uua0', 'uua1', 'uua2', 'uua3', 'uua4', 'uua5', 'v0', 'v1', 'v2', 'v3', 'v4', 'v5', 'vv0', 'vv1', 'vv2', 'vv3', 'vv4', 'vv5', 'vva0', 'vva1', 'vva2', 'vva3', 'vva4', 'vva5', 'w', 'w^', 'x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'xx0', 'xx1', 'xx2', 'xx3', 'xx4', 'xx5', 'y', 'z']

  char_repeats: 1                # Number of grapheme character repeats to allow for mapping to longer phoneme sequences.
                                 # Set to 1 for autoreg_transformer.
  lowercase: false                # Whether to lowercase the grapheme input.
  n_val: 100                    # Default number of validation data points if no explicit validation data is provided.


model:
  type: 'autoreg_transformer'        # Whether to use a forward transformer or autoregressive transformer model.
                                     # Choices: ['transformer', 'autoreg_transformer']
  d_model: 256
  d_fft: 512
  layers: 3
  dropout: 0.1
  heads: 4

training:

  # Hyperparams for learning rate and scheduler.
  # The scheduler is reducing the lr on plateau of phoneme error rate (tested every n_generate_steps).

  learning_rate: 0.0001              # Learning rate of Adam.
  warmup_steps: 1000                 # Linear increase of the lr from zero to the given lr within the given number of steps.
  scheduler_plateau_factor: 0.5      # Factor to multiply learning rate on plateau.
  scheduler_plateau_patience: 10     # Number of text generations with no improvement to tolerate.
  batch_size: 128                     # Training batch size.
  batch_size_val: 64                 # Validation batch size.
  epochs: 1000                        # Number of epochs to train.
  generate_steps: 1000              # Interval of training steps to generate sample outputs. Also, at this step the phoneme and word
                                     # error rates are calculated for the scheduler.
  validate_steps: 1000              # Interval of training steps to validate the model
                                     # (for the autoregressive model this is teacher-forced).
  checkpoint_steps: 1000            # Interval of training steps to save the model.
  n_generate_samples: 50             # Number of result samples to show on tensorboard.
  store_phoneme_dict_in_model: true  # Whether to store the raw phoneme dict in the model.
                                     # It will be loaded by the phonemizer object.

