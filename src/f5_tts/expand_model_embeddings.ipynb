{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba35c85-2c3d-4ed6-be55-4df7ddedaace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "from importlib.resources import files\n",
    "from cached_path import cached_path\n",
    "\n",
    "path_data = str(files(\"f5_tts\").joinpath(\"../../data\"))\n",
    "path_project_ckpts = str(files(\"f5_tts\").joinpath(\"../../ckpts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4164fbc7-f415-41d8-aaa8-b1465b79a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_model_embeddings(ckpt_path, new_ckpt_path, num_new_tokens=42):\n",
    "    seed = 666\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    if ckpt_path.endswith(\".safetensors\"):\n",
    "        ckpt = load_file(ckpt_path, device=\"cpu\")\n",
    "        ckpt = {\"ema_model_state_dict\": ckpt}\n",
    "    elif ckpt_path.endswith(\".pt\"):\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    ema_sd = ckpt.get(\"ema_model_state_dict\", {})\n",
    "    embed_key_ema = \"ema_model.transformer.text_embed.text_embed.weight\"\n",
    "    old_embed_ema = ema_sd[embed_key_ema]\n",
    "\n",
    "    vocab_old = old_embed_ema.size(0)\n",
    "    embed_dim = old_embed_ema.size(1)\n",
    "    vocab_new = vocab_old + num_new_tokens\n",
    "\n",
    "    def expand_embeddings(old_embeddings):\n",
    "        new_embeddings = torch.zeros((vocab_new, embed_dim))\n",
    "        new_embeddings[:vocab_old] = old_embeddings\n",
    "        new_embeddings[vocab_old:] = torch.randn((num_new_tokens, embed_dim))\n",
    "        return new_embeddings\n",
    "\n",
    "    ema_sd[embed_key_ema] = expand_embeddings(ema_sd[embed_key_ema])\n",
    "\n",
    "    if new_ckpt_path.endswith(\".safetensors\"):\n",
    "        save_file(ema_sd, new_ckpt_path)\n",
    "    elif new_ckpt_path.endswith(\".pt\"):\n",
    "        torch.save(ckpt, new_ckpt_path)\n",
    "\n",
    "    return vocab_new\n",
    "    \n",
    "def vocab_extend(project_name, model_type=\"F5TTS_v1_Base\"):\n",
    "    name_project = project_name\n",
    "    path_project = os.path.join(path_data, name_project)\n",
    "    file_vocab_project = os.path.join(path_project, \"vocab.txt\")\n",
    "\n",
    "    file_vocab = os.path.join(path_data, \"Emilia_ZH_EN_pinyin/vocab.txt\")\n",
    "    if not os.path.isfile(file_vocab):\n",
    "        return f\"the file {file_vocab} not found !\"\n",
    "\n",
    "    # project vocab\n",
    "    symbols = []\n",
    "    with open(file_vocab_project, 'r') as f:\n",
    "        data = f.read()\n",
    "        symbols = data.split(\"\\n\")\n",
    "    if symbols == []:\n",
    "        return \"Symbols to extend not found.\"\n",
    "    # print(f\"project vocab: {len(symbols)}\")\n",
    "    \n",
    "    # F5 v1 base vocab\n",
    "    with open(file_vocab, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = f.read()\n",
    "        vocab = data.split(\"\\n\")\n",
    "    vocab_check = set(vocab)\n",
    "    # print(f\"project vocab: {len(vocab_check)}\")\n",
    "\n",
    "    miss_symbols = []\n",
    "    for item in symbols:\n",
    "        item = item.replace(\" \", \"\")\n",
    "        if item in vocab_check:\n",
    "            continue\n",
    "        miss_symbols.append(item)\n",
    "\n",
    "    if miss_symbols == []:\n",
    "        return \"Symbols are okay no need to extend.\"\n",
    "    # print(f\"missing vocab: {len(miss_symbols)}\")\n",
    "\n",
    "    size_vocab = len(vocab)\n",
    "    vocab.pop()\n",
    "    for item in miss_symbols:\n",
    "        vocab.append(item)\n",
    "    vocab.append(\"\")\n",
    "    print(f\"project vocab: {len(vocab)}\")\n",
    "\n",
    "    # we have done this manually, no need to rewrite this again\n",
    "    with open(file_vocab_project, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(vocab))\n",
    "\n",
    "    if model_type == \"F5TTS_v1_Base\":\n",
    "        ckpt_path = str(cached_path(\"hf://SWivid/F5-TTS/F5TTS_v1_Base/model_1250000.safetensors\"))\n",
    "    elif model_type == \"F5TTS_Base\":\n",
    "        ckpt_path = str(cached_path(\"hf://SWivid/F5-TTS/F5TTS_Base/model_1200000.pt\"))\n",
    "    elif model_type == \"E2TTS_Base\":\n",
    "        ckpt_path = str(cached_path(\"hf://SWivid/E2-TTS/E2TTS_Base/model_1200000.pt\"))\n",
    "\n",
    "    vocab_size_new = len(miss_symbols)\n",
    "\n",
    "    dataset_name = name_project.replace(\"_pinyin\", \"\").replace(\"_char\", \"\")\n",
    "    new_ckpt_path = os.path.join(path_project_ckpts, dataset_name)\n",
    "    os.makedirs(new_ckpt_path, exist_ok=True)\n",
    "\n",
    "    # Add pretrained_ prefix to model when copying for consistency with finetune_cli.py\n",
    "    new_ckpt_file = os.path.join(new_ckpt_path, \"pretrained_\" + os.path.basename(ckpt_path))\n",
    "\n",
    "    size = expand_model_embeddings(ckpt_path, new_ckpt_file, num_new_tokens=vocab_size_new)\n",
    "\n",
    "    vocab_new = \"\\n\".join(miss_symbols)\n",
    "    print(f\"vocab old size : {size_vocab}\\nvocab new size : {size}\\nvocab add : {vocab_size_new}\\nnew symbols :\\n{vocab_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59806a8b-faab-4d3e-bd4d-2beee3ad18a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project vocab: 2587\n",
      "vocab old size : 2546\n",
      "vocab new size : 2587\n",
      "vocab add : 41\n",
      "new symbols :\n",
      "^\n",
      "ค\n",
      "ฆ\n",
      "ฉ\n",
      "ช\n",
      "ซ\n",
      "ญ\n",
      "ฎ\n",
      "ฏ\n",
      "ฐ\n",
      "ฑ\n",
      "ฒ\n",
      "ณ\n",
      "ด\n",
      "ถ\n",
      "ธ\n",
      "บ\n",
      "ผ\n",
      "ฝ\n",
      "พ\n",
      "ฟ\n",
      "ภ\n",
      "ม\n",
      "ฤ\n",
      "ล\n",
      "ศ\n",
      "ษ\n",
      "ฬ\n",
      "ฯ\n",
      "ะ\n",
      "ำ\n",
      "ิ\n",
      "ื\n",
      "ุ\n",
      "ู\n",
      "เ\n",
      "แ\n",
      "ๅ\n",
      "็\n",
      "๊\n",
      "๋\n"
     ]
    }
   ],
   "source": [
    "project_name = \"custom_th_pinyin\"\n",
    "vocab_extend(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b98a40f-c010-4d8b-9759-239c8dccea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/lt200249-speech/hall/datasets/multi-tts/en/emilia/wavs/EN_B00074_S06852_W000034.json\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "test = Path('/project/lt200249-speech/hall/datasets/multi-tts/en/emilia/wavs/EN_B00074_S06852_W000034.json')\n",
    "print(test)\n",
    "\n",
    "if os.path.exists(test):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06cffa10-52dc-4b3f-b6f6-533f33203900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/lt200249-speech/hall/datasets/multi-tts/en/emilia/wavs/EN_B00074_S06852_W000034.mp3\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "test2 = test.with_suffix('.mp3')\n",
    "print(test2)\n",
    "if os.path.exists(test2):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb00957-db0f-46ac-a1fa-19421dca9bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-f5tts]",
   "language": "python",
   "name": "conda-env-.conda-f5tts-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
